{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will implement the machine learning models needed to predict house sale price.  The following machine learning models including a lasso regression, ridge regression, elastic net regression, a random forest model, gradient boosted models, a stacked, and a blended model will be implemented.  The output training RMSE of the models will be compared and the model with the lowest error will predict on the test set.  The resulting predictions of the best model will be submitted to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Koalas library was used when interacting with big data because it implements the Pandas DataFrame API on top of Apache Spark. Pandas is typically used for single node Dataframe implementation in Python while Spark is used for big data processing. With Koalas, Pandas syntax can be used while processing dataframes with Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.library.installPyPI(\"koalas\")\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be evaluated on RMSE which is the Root Mean Squared Error between the prediction and the label.  When the dataframe is loaded, some of the columns that are numeric should be strings as string format is more accurate for values that are not ordered or increasing in value.  An extra field was created called YrBltAndRemod that is the sum of the YearBuilt and the YearRemodAdd features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import numpy as np\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "full_df=pd.read_csv('/dbfs/FileStore/cleaned1.csv', index_col=0)\n",
    "train_label=pd.read_csv('/dbfs/FileStore/trainlabel.csv', header=None)\n",
    "\n",
    "full_df['MSSubClass']=full_df['MSSubClass'].astype(str)\n",
    "full_df['MoSold']=full_df['MoSold'].astype(str)\n",
    "full_df['YrSold']=full_df['YrSold'].astype(str)\n",
    "\n",
    "full_df['YrBltAndRemod']=full_df['YearBuilt']+full_df['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric features were collected so that they could be transformed to normal distributions.  A normal distrubtion helps a machine learning model evaluate the samples because outliers will not be as unevely distributed once the feature  is transformed to a normal distribution.  If the value of skew was above 0.5, the boxcox1p transformation was applied.  The transformaion is involves taking the input value to exponent of the optimal lambda, subtracting one, and then dividing by lambda.  This occurs when lambda is not 0 and when lambda is equal to 0, the log of the values of the feature are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/scipy/stats/stats.py:3038: RuntimeWarning: invalid value encountered in double_scalars\n",
       "  r = r_num / r_den\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numeric_features=[]\n",
    "for i in full_df.columns:\n",
    "    if full_df[i].dtype!=object:\n",
    "        numeric_features.append(i)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "skew_features=full_df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew=skew_features=skew_features[skew_features>0.5]\n",
    "skew_index=high_skew.index\n",
    "\n",
    "for i in skew_index:\n",
    "  full_df[i]=boxcox1p(full_df[i], boxcox_normmax(full_df[i]+1))\n",
    "\n",
    "  \n",
    "scaler = RobustScaler()\n",
    "\n",
    "full_df[numeric_features] = scaler.fit_transform(full_df[numeric_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset was split into train and test sets.  The train data was subsetted to only include samples with a GrLivArea less than 4500 as whese values were found to be outliders.  Becuase the models will be evaulation on difference of the log RMSE and labels, the labels were transformed by taking the log of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df=full_df[:1460]\n",
    "test_df=full_df[1460:]\n",
    "\n",
    "\n",
    "train_df = train_df[train_df.GrLivArea < 4500]\n",
    "\n",
    "train_df['label']=np.log1p(train_label[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark dataframes were createad from the test, train and full dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(2919, 90)\n",
       "(1460, 91)\n",
       "(1459, 90)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=train_df\n",
    "X_test=test_df\n",
    "X_dropped=X.drop(['label'], axis=1)\n",
    "\n",
    "X=pd.concat([X_dropped, X_test])\n",
    "X=pd.get_dummies(X)\n",
    "\n",
    "X_test=X[1460:]\n",
    "X=X[:1460]\n",
    "\n",
    "\n",
    "y=np.log1p(train_label[1])\n",
    "\n",
    "\n",
    "train_df = spark.createDataFrame(train_df)\n",
    "full_df=spark.createDataFrame(full_df)\n",
    "test_df = spark.createDataFrame(test_df)\n",
    "\n",
    "print((full_df.count(), len(full_df.columns)))\n",
    "print((train_df.count(), len(train_df.columns)))\n",
    "print((test_df.count(), len(test_df.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to convert the categorical variables in the dataset into numeric variables. There are 2 steps to do this.\n",
    "\n",
    "\n",
    "The below code basically indexes each categorical column using the StringIndexer, and then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "categoricalColumns=[]\n",
    "for i in full_df.columns:\n",
    "  if isinstance(full_df.schema[i].dataType, StringType):\n",
    "    categoricalColumns.append(i)\n",
    "\n",
    "stages=[]  #stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "  stringIndexer=StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "  \n",
    "  encoder=OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol+\"classVec\"], dropLast=False)\n",
    "  \n",
    "  stages += [stringIndexer, encoder]\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group together numerical columns and then use a VectorAssembler to combine all the feature columns, categorical and numerica, into a single vector column. This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numericCols=[]\n",
    "for i in full_df.columns:\n",
    "  if i==\"label\":\n",
    "    continue\n",
    "  if isinstance(full_df.schema[i].dataType, StringType)==False:\n",
    "    numericCols.append(i)\n",
    "   \n",
    "    \n",
    "\n",
    "assemblerInputs=[c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler=VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the stages as a Pipeline. This puts the data through all of the feature transformations we described in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "\n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel=partialPipeline.fit(full_df)\n",
    "preppedDataDF=pipelineModel.transform(train_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is simlilary Run through the stages as a Pipeline. This puts the data through all of the feature transformations we described in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel=partialPipeline.fit(full_df)\n",
    "preppedDataDF_test=pipelineModel.transform(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal hyperparameters are found using the hyperopt package which implements bayesian optimizaiton.  \n",
    "\n",
    "Bayesian optimizaiton can be explained in four steps:\n",
    "1.) Initialize process by sampling the hyperparameter space either randomly or low-discrepancy sequencing and getting these observations. \n",
    "\n",
    "2.) In this case, fit a Gaussian process to the observed data from step 1. Use the mean from the Gaussian process as the function most likely to model the black box function.\n",
    "\n",
    "3.) Use the maximal location of the acquisition function to figure out where to sample next in the hyperparameter space. \n",
    "\n",
    "4.) Get an observation of the black box function given the newly sampled hyperparameter points. Add observations to the set of observed data.\n",
    "\n",
    "\n",
    "This process (Steps 2-4) repeats until a maximum number of iterations is met. By iterating through the method explicated above, Bayesian optimization effectively searches the hyperparameter space while homing in on the global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt\n",
    "from hyperopt import fmin, hp, tpe\n",
    "from hyperopt import SparkTrials, STATUS_OK\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from functools import partial\n",
    "\n",
    "search_space_ridge = {\n",
    "  'alphas': hp.choice('alphas', [14.5, 15, 15.1, 15.2, 15.3, 15.4]),\n",
    "  'max_iter': hp.choice('max_iter', [1e7])\n",
    "}\n",
    "\n",
    "search_space_lasso = {\n",
    "  'alphas': hp.choice('alphas', [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]),\n",
    "  'max_iter': hp.choice('max_iter', [1e7])\n",
    "\n",
    "}\n",
    "\n",
    "search_space_elastic = {\n",
    "  'alphas': hp.choice('alphas', [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]),\n",
    "  'max_iter': hp.choice('max_iter', [1e7]),\n",
    "  'l1_ratio': hp.choice('l1_ratio', [1])\n",
    "}\n",
    "\n",
    "algo=partial(tpe.suggest, n_startup_jobs=1, n_EI_candidates=1)\n",
    "\n",
    "rstate = np.random.RandomState(42)\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "def train_ridge(params):\n",
    "  clf=RidgeCV(cv=10)\n",
    "  score = cv_rmse(clf).mean()\n",
    "  return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def train_lasso(params):\n",
    "  clf=LassoCV(cv=10)\n",
    "  score = cv_rmse(clf).mean()\n",
    "  return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def train_elastic(params):\n",
    "  clf=ElasticNetCV(cv=10)\n",
    "  score = cv_rmse(clf).mean()\n",
    "  return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "best_hyperparameters_ridge = fmin(\n",
    "        fn=train_ridge,\n",
    "        space=search_space_ridge,\n",
    "        algo=algo,\n",
    "        max_evals=10,\n",
    "        return_argmin=False,\n",
    "        )\n",
    "\n",
    "print('Ridge HyperParameters: ' + str(best_hyperparameters_ridge))\n",
    "'''Ridge HyperParameters: {'alphas': 14.5, 'max_iter': 10000000.0}'''\n",
    "\n",
    "\n",
    "best_hyperparameters_lasso = fmin(\n",
    "  fn=train_lasso,\n",
    "  space=search_space_lasso,\n",
    "  algo=algo,\n",
    "  max_evals=12,\n",
    "  return_argmin=False\n",
    "  )\n",
    "\n",
    "print('Lasso HyperParameters: ' + str(best_hyperparameters_lasso))\n",
    "\n",
    "\n",
    "'''Lasso HyperParameters: {'alphas': 0.0008, 'max_iter': 10000000.0}'''\n",
    "\n",
    "best_hyperparameters_elastic = fmin(\n",
    "  fn=train_elastic,\n",
    "  space=search_space_elastic,\n",
    "  algo=algo,\n",
    "  max_evals=20,\n",
    "  \n",
    "  return_argmin=False\n",
    "  )\n",
    "\n",
    "\n",
    "print('Elastic HyperParameters: ' + str(best_hyperparameters_elastic))\n",
    "'''Elastic HyperParameters: {'alphas': 0.0007, 'l1_ratio': 1, 'max_iter': 10000000.0}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the optimal hyperparameters from bayesian optimization were found, we can build small hyperparameter grids with only one set of hyperparameters that can be searched quickly, which will lower training times.  \n",
    "\n",
    "\n",
    "Linear regression predicts the response variable in this case home sale price as the dot product of p distinct predictors and their corresponding coefficients coefficients.  An intercept term is added to the dot product so the response variable has a non zero value when all of the preditor values are zero.  In this case home sale price should have an intercept term because home price must be greater than 0.  The multiple linear regression model takes the form\n",
    "Y = β0 + β1X1 + β2X2 + ··· + βpXp + ... βj can be interpreted as the increase in the response variable with one unit increase of Xj assuming all other slope coefficients are set to zero.  \n",
    "\n",
    "\n",
    "Three different estimators were tested: ridge regression, lasso regression, and elastic net regression.  These three types of regression include different regularization types. In the final report, the equations for the types of regression with regularization will be listed.  The purpose of regularization is to prevent overfitting by adding penalty terms that cause the loss function to have less predictor terms. \n",
    "\n",
    "\n",
    "In order to implement Ridge regression, the elastic parameter must be set to zero which cancels out the L1 term.  To implement lasso regression, the elastic parameter must be 1 to cancel out the L2 term.  To implement elastic net, the elastic net parameter as well as the L1 term must be greater than 0 and less than 1 but 1 was the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regEval = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                 labelCol=\"label\", metricName = \"rmse\")\n",
    "\n",
    "lr=LinearRegression(labelCol=\"label\", featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "\n",
    "#ridge\n",
    "\n",
    "paramGrid_ridge = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [14.5])\n",
    "             .addGrid(lr.elasticNetParam, [0])\n",
    "             .addGrid(lr.maxIter, [1e7])\n",
    "             .build())\n",
    "\n",
    "#lasso\n",
    "paramGrid_lasso = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.008])\n",
    "             .addGrid(lr.elasticNetParam, [1])\n",
    "             .addGrid(lr.maxIter, [1e7])\n",
    "             .build())\n",
    "\n",
    "#elastic net\n",
    "\n",
    "paramGrid_elastic = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0007])\n",
    "             .addGrid(lr.elasticNetParam, [1])\n",
    "             .addGrid(lr.maxIter, [1e7])\n",
    "             .build())\n",
    "\n",
    "\n",
    "\n",
    "#elasticnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation was performed on the training set with 10 folds and evaluated using the RMSE.  The fitted models were saved in order to save time so that they do not need to be rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_ridge, evaluator=regEval, numFolds=10)\n",
    "cvModel_ridge_ = cv.fit(preppedDataDF)\n",
    "\n",
    "cvModel_ridge_.write().overwrite().save('myModel_ridge')\n",
    "cvModel_ridge=CrossValidatorModel.load('myModel_ridge')\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_lasso, evaluator=regEval, numFolds=10)\n",
    "cvModel_lasso_ = cv.fit(preppedDataDF)\n",
    "\n",
    "cvModel_lasso_.write().overwrite().save('myModel_lasso')\n",
    "cvModel_lasso=CrossValidatorModel.load('myModel_lasso')\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid_elastic, evaluator=regEval, numFolds=10)\n",
    "cvModel_elastic_ = cv.fit(preppedDataDF)\n",
    "\n",
    "\n",
    "cvModel_elastic_.write().overwrite().save('myModel_elastic')\n",
    "cvModel_elastic=CrossValidatorModel.load('myModel_elastic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following metrics were outputted of the model: The intercept of the linear regression function, the top 15 features with the highest coefficients, the optimal parameters of the model, the number of iterations the model took, the best training RMSE, the r^2 value of the linear regression fit, and the RMSE of the validation set for the best parameter combination.  The largest coefficients tell us what can be changed about the house to increase the house sale price.\n",
    "\n",
    "\n",
    "The elastic net model recieved a validation score of 0.1284 and the ridge regression model recieved a validation score of 0.2808.  The optimal hyperparameters in the elastic net model removed the L2 term which means Lasso  must have performed better than ridge regression.    The lasso regression model performed the best with a validation RMSE of 0.1250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics(cvModel, model):\n",
    "\n",
    "  coeff=cvModel.bestModel.coefficients\n",
    "  print(\"Intercept: %s\" % str(cvModel.bestModel.intercept))\n",
    "  # Summarize the model over the training set and print out some metrics\n",
    "  feature_names=(preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"]+preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"])\n",
    "\n",
    "  features=[]\n",
    "  for i in range(len(feature_names)):\n",
    "    features.append(feature_names[i]['name'])\n",
    "\n",
    "  coeffs=list(zip(features, coeff))\n",
    "  coeffs = sorted(coeffs, key=lambda tup: tup[1], reverse=True)[0:15]\n",
    "  print(coeffs)\n",
    "\n",
    "  #trainingSummary = cvModel_lasso_.bestModel.summary\n",
    "\n",
    "  best_reg_param = cvModel.bestModel._java_obj.getRegParam()\n",
    "  print('Regularization Parameter: ' + str(best_reg_param))\n",
    "  if cvModel==cvModel_elastic:\n",
    "    best_elastic_param = cvModel.bestModel._java_obj.getElasticNetParam()\n",
    "    print('ElasticNet Parameter: ' + str(best_elastic_param))\n",
    "  \n",
    "  \n",
    "  trainingSummary = model.bestModel.summary\n",
    "  print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "  print('RMSE: {}'.format(trainingSummary.rootMeanSquaredError))\n",
    "  print(\"r2: %f\" % trainingSummary.r2)\n",
    "  print('Average Metrics for each Parameter Combination: ' + str(model.avgMetrics))\n",
    "  print('\\n\\n')\n",
    "\n",
    "metrics(cvModel_ridge, cvModel_ridge_) \n",
    "metrics(cvModel_lasso, cvModel_lasso_)\n",
    "metrics(cvModel_elastic, cvModel_elastic_) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Intercept: 11.977677521849378\n",
    "Top Features: [('PoolQCclassVec_Ex', 0.019000667896077787), ('RoofMatlclassVec_WdShngl', 0.01602273617277807), ('Condition2classVec_PosA', 0.01543805075340896), ('ExterQualclassVec_Ex', 0.01421741838657165), ('NeighborhoodclassVec_NoRidge', 0.012666748145105981), ('FireplaceQuclassVec_Ex', 0.012527127269855954), ('KitchenQualclassVec_Ex', 0.01235360560865593), ('BsmtQualclassVec_Ex', 0.012217563394786614), ('Exterior2ndclassVec_Other', 0.011434343474303024), ('NeighborhoodclassVec_NridgHt', 0.01131313661231901), ('NeighborhoodclassVec_StoneBr', 0.011026366215851235), ('CentralAirclassVec_Y', 0.009440796647356895), ('SaleTypeclassVec_Con', 0.009303717513164412), ('Exterior1stclassVec_Stone', 0.009264673590399534), ('hasgarage', 0.009208381063546765)]\n",
    "Regularization Parameter: 14.5\n",
    "numIterations: 1\n",
    "RMSE: 0.27948058559447003\n",
    "r2: 0.510134\n",
    "Average Validation RMSE for best Parameter Combination: [0.2808121355653476]\n",
    "\n",
    "\n",
    "\n",
    "Intercept: 11.963706867662323\n",
    "Top Features: [('Total_SF', 0.1059140989389088), ('GrLivArea', 0.08357871462008833), ('Total_Home_Quality', 0.07938970656123005), ('NeighborhoodclassVec_Crawfor', 0.07244816040350494), ('YrBltAndRemod', 0.07063186115311705), ('NeighborhoodclassVec_StoneBr', 0.05892897856487002), ('KitchenQualclassVec_Ex', 0.055302642687728096), ('BsmtQualclassVec_Ex', 0.0497972409120521), ('NeighborhoodclassVec_NoRidge', 0.04283947669470563), ('Exterior1stclassVec_BrkFace', 0.041378198106361515), ('BsmtFinSF1', 0.04072661981812749), ('FunctionalclassVec_Typ', 0.03843061897704698), ('OverallQual', 0.03716151956168724), ('GarageCars', 0.034263181390235195), ('SaleTypeclassVec_New', 0.03384804680101499)]\n",
    "Regularization Parameter: 0.008\n",
    "numIterations: 294\n",
    "RMSE: 0.1088871197501967\n",
    "r2: 0.925642\n",
    "Average Validation RMSE for best Parameter Combination: [0.125056342529829]\n",
    "\n",
    "\n",
    "\n",
    "Intercept: 11.813980776889892\n",
    "Top Features: [('GarageQualclassVec_Ex', 0.24681017986441736), ('RoofMatlclassVec_Membran', 0.23057278879163012), ('Condition2classVec_PosA', 0.1735019582510545), ('PoolQCclassVec_Gd', 0.14835817467259307), ('RoofStyleclassVec_Shed', 0.14083215165847704), ('NeighborhoodclassVec_StoneBr', 0.13688117779597792), ('Total_SF', 0.11829057573914872), ('NeighborhoodclassVec_Crawfor', 0.11157239563318498), ('RoofMatlclassVec_Metal', 0.11113008836938372), ('RoofMatlclassVec_WdShngl', 0.1016673278090774), ('PoolQCclassVec_Ex', 0.09944854240169966), ('SaleTypeclassVec_ConLD', 0.09016653084130574), ('GrLivArea', 0.08190836070021754), ('NeighborhoodclassVec_NoRidge', 0.08040881506696701), ('Exterior1stclassVec_BrkFace', 0.07814012472224557)]\n",
    "Regularization Parameter: 0.0007\n",
    "ElasticNet Parameter: 1.0\n",
    "numIterations: 1224\n",
    "RMSE: 0.09158737568716792\n",
    "r2: 0.947393\n",
    "Average Validation RMSE for Best Parameter Combination: [0.12843334246807203]\n",
    "\n",
    "'''\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier is an ensemble method that uses many decision trees that have had bootstrap aggregating or bagging done on the samples. Bagging means that samples are chosen at random with replacement to reduce variance in any single tree and the results are averaged. The prediction in an individual tree is the average value of the response variable for the samples of a leaf node.  The prediction of the bagged regression decision tree  is the average of the predictions for each individual decision tree.  \n",
    "\n",
    "Random forests in addition take a subset of the predictors when constructing a tree so that each tree is more different from one another. The combination of the trees made from bagged samples and subsetted predictors is the random forest result. The random forest model was the worst performing ensemble method with a validation RMSE of 0.1682."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel \n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "\n",
    "regEval = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                 labelCol=\"label\", metricName = \"rmse\")\n",
    "\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "             .addGrid(rf.featureSubsetStrategy, ['sqrt'])\n",
    "             .addGrid(rf.maxDepth, [5])\n",
    "             .addGrid(rf.numTrees, [3000])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid_rf, evaluator=regEval, numFolds=10)\n",
    "\n",
    "pipeline=Pipeline(stages=[cv])\n",
    "cvModel_rf_ = cv.fit(preppedDataDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_rf_.write().overwrite().save('myModel_rf')\n",
    "cvModel_rf=CrossValidatorModel.load('myModel_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(cvModel, model):\n",
    "\n",
    "  feat_imp=cvModel.bestModel.featureImportances\n",
    "\n",
    "  feature_names=(preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"]+preppedDataDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"])\n",
    "\n",
    "  features=[]\n",
    "  for i in range(len(feature_names)):\n",
    "    features.append(feature_names[i]['name'])\n",
    "\n",
    "  feat_imp=list(zip(features, feat_imp))\n",
    "  feat_imp = sorted(feat_imp, key=lambda tup: tup[1], reverse=True)[0:15]\n",
    "  print(feat_imp)\n",
    "  \n",
    "  print('Average Validation RMSE for Best Parameter Combination: ' + str(model.avgMetrics))\n",
    "\n",
    "  \n",
    "metrics(cvModel_rf, cvModel_rf_) \n",
    "\n",
    "\n",
    "'''Most Important features: [('Total_SF', 0.07942801665560263), ('OverallQual', 0.0762820067958877), ('GrLivArea', 0.05705986955640432), ('Total_bath', 0.03945939278404038), ('YearBuilt', 0.03886143544353758), ('YrBltAndRemod', 0.03878246729733818), ('GarageCars', 0.03732368466637657), ('Total_Home_Quality', 0.03413133362982473), ('ExterQualclassVec_TA', 0.03300783939600958), ('GarageArea', 0.03262434890655314), ('TotalBsmtSF', 0.031826775360823144), ('FullBath', 0.02902317536062917), ('1stFlrSF', 0.026346867027809858), ('GarageYrBlt', 0.024781569363908476), ('YearRemodAdd', 0.02113836596814282)]\n",
    "Average Validation RMSE for Best Parameter Combination: [0.16819963092531479]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gradient boosted classifier involves combining a large number of trees with bagging and boosting. Each subsequent tree that is constructed is done so based on the least pure splits created by the previous tree. The trees are added back to the previous tree so that the tree can learn slowly and improve RMSE. An advantage to gradient boosted trees is that compared to a decision tree that may overfit to one set of the data because gradient boosted trees learn by focusing on the worst split in a tree and improve over time. The disadvantages of gradient boosted trees is that they may overfit compared to random forests but not in this case.\n",
    "\n",
    "\n",
    "The gradient boosted classifier had an average valdiation RMSE of 0.1208 on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "clf=GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)      \n",
    "score = cv_rmse(clf).mean()\n",
    "print('Average Validation Loss: ' + str(score))\n",
    "\n",
    "print('\\033[1m' + 'Top 10 features in gradient boosted tree that contribute to home sale price:' + '\\033[0m')\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid={}, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "gbt=gsc.fit(X, y)\n",
    "\n",
    "y_pred_gbr=gbt.predict(X_test)\n",
    "\n",
    "print('Top 10 features in gradient boosted tree that contribute to home sale price:')\n",
    "sorted(list(zip(X.columns, gbt.best_estimator_.feature_importances_)), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "\n",
    "'''Average Validation Loss: 0.12079922229921425\n",
    "Top 10 features in gradient boosted tree that contribute to home sale price:\n",
    "Top 10 features in gradient boosted tree that contribute to home sale price:\n",
    "Out[19]: [('GrLivArea', 0.13141904142177038),\n",
    " ('Total_SF', 0.08782575198710837),\n",
    " ('OverallQual', 0.06680771059701276),\n",
    " ('TotalBsmtSF', 0.06581417875915589),\n",
    " ('YrBltAndRemod', 0.04300053789530215),\n",
    " ('GarageArea', 0.04141307756162639),\n",
    " ('LotArea', 0.03350685544843198),\n",
    " ('Fireplaces', 0.03295994764788792),\n",
    " ('ExterQual_Gd', 0.029057445867734944),\n",
    " ('Total_Home_Quality', 0.027583003487325338)]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lightgbm is a type of gradient boosting regressors in that new trees are learning from the worst splits in the previous tree.  One key difference is that lightgbm regressor grow leaf wise as opposed to level wise of gradient boosting regressors.  Leaf wise growth constructs a new tree from a leaf new and keeps creating new trees from the leaf nodes of the previously created trees.  Level wise growth must have all the leaves on a level of a tree split into new nodes between the resulting nodes can be split.  \n",
    "\n",
    "Both lighgbm and xgboost use histogram based bins to split the data of an attribute into bins.  The decision tree only needs to check the number of bins for each attribute.  \n",
    "\n",
    "The lightgbm model achieved an average validation loss 0.1207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "clf=LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       )   \n",
    "\n",
    "score = cv_rmse(clf).mean()\n",
    "print('Average Validation Loss: ' + str(score))\n",
    "\n",
    "print('\\033[1m' + 'Top 10 features in light gradient boosted tree that contribute to home sale price:' + '\\033[0m')\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid={}, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "gbt=gsc.fit(X, y)\n",
    "\n",
    "y_pred_gbr=gbt.predict(X_test)\n",
    "\n",
    "print('Top 10 features in gradient boosted tree that contribute to home sale price:')\n",
    "sorted(list(zip(X.columns, gbt.best_estimator_.feature_importances_)), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "'''Average Validation Loss: 0.12072925676713671\n",
    "Top 10 features in light gradient boosted tree that contribute to home sale price:\n",
    "Top 10 features in gradient boosted tree that contribute to home sale price:\n",
    "Out[20]: [('LotArea', 651),\n",
    " ('GrLivArea', 598),\n",
    " ('Total_SF', 594),\n",
    " ('1stFlrSF', 572),\n",
    " ('GarageArea', 501),\n",
    " ('TotalBsmtSF', 487),\n",
    " ('Total_porch_sf', 463),\n",
    " ('LotFrontage', 445),\n",
    " ('YrBltAndRemod', 425),\n",
    " ('YearBuilt', 402)]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xgboost is a type of gradient boosting regressors in that new trees are learning from the worst splits in the previous tree.  One key difference is that xgboost regressor grow leaf wise as opposed to level wise of gradient boosting regressors.  Leaf wise growth constructs a new tree from a leaf new and keeps creating new trees from the leaf nodes of the previously created trees.  Level wise growth must have all the leaves on a level of a tree split into new nodes between the resulting nodes can be split.  \n",
    "\n",
    "Both lighgbm and xgboost use histogram based bins to split the data of an attribute into bins.  The decision tree only needs to check the number of bins for each attribute.  XGBoost is also known as the regularised version of GBM. This framework includes built-in L1 and L2 regularisation which means it can prevent a model from overfitting.\n",
    "\n",
    "The xgboost model achieved an average validation loss was 0.1166."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "clf=XGBRegressor(learning_rate=0.01,n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:squarederror', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)\n",
    "\n",
    "score = cv_rmse(clf).mean()\n",
    "print('Average Validation Loss: ' + str(score))\n",
    "\n",
    "gbt = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid={}, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "gbt.fit(X, y)\n",
    "\n",
    "y_pred_xboost=gbt.predict(X_test)\n",
    "\n",
    "#sorted(list(zip(X.columns, gbt.best_estimator_.feature_importances_)), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "'''Average Validation Loss: 0.11659982710488989\n",
    "Top 10 features in xg-boosted tree that contribute to home sale price:\n",
    "Out[21]: [('GarageCars', 0.053317003),\n",
    " ('hasfireplace', 0.052065946),\n",
    " ('Total_SF', 0.050921544),\n",
    " ('Total_bath', 0.034195196),\n",
    " ('BsmtQual_Ex', 0.03344998),\n",
    " ('OverallQual', 0.03156224),\n",
    " ('Fireplaces', 0.03149775),\n",
    " ('YrBltAndRemod', 0.031276856),\n",
    " ('CentralAir_Y', 0.024429448),\n",
    " ('Total_Home_Quality', 0.022839572)]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking enables a method of ensembling multiple regression models. \n",
    "\n",
    "The training data is split into K-folds and a base model is fitted on the K-1 parts.  A prediction is made for the Kth part and repeated for each part of the train set.  The base model is then fitted on the whole train dataset.  The previous steps are repeated for all of the base models.  The predictions of the base models from the k-fold cross validation of the train set are used as features in the meta-regressor model.  This second level model is used to make final predictions on the test set which has added features from the prediction made from the fitted base model predictions on the test set.\n",
    "\n",
    "As a side note, blending is similar to stacking bt uses a training and validation split instead of k-fold cross validation.  The average validaion RMSE of the model was 0.1195."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[8]: &#34;&#39;stack = GridSearchCV(\\n       estimator=stack_gen, param_grid={},\\n        cv=10, scoring=&#39;neg_mean_squared_error&#39;)\\n\\n\\nstack.fit(X, y)\\nStack score with lightgbm: 0.1195, \\ny_pred_xboost=stack.predict(X_test)&#34;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install mlxtend\n",
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "  \n",
    "def cv_rmse(model, X=X):\n",
    "\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)\n",
    "  \n",
    "ridge = RidgeCV(alphas=[14.5], cv=kfolds)\n",
    "lasso = LassoCV(max_iter=1e7, alphas=[0.0008], random_state=42, cv=kfolds)\n",
    "\n",
    "elasticnet = ElasticNetCV(max_iter=1e7, alphas=[0.0007],\n",
    "                                        cv=kfolds, l1_ratio=[1])\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "                                   \n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       #min_data_in_leaf=2,\n",
    "                                       #min_sum_hessian_in_leaf=11\n",
    "                                       )\n",
    "                                       \n",
    "xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:squarederror', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)\n",
    "\n",
    "\n",
    "# stack\n",
    "stack_gen = StackingCVRegressor(regressors=(lasso, lightgbm, xgboost, gbr, elasticnet, lasso),\n",
    "                                meta_regressor=lightgbm,\n",
    "                                use_features_in_secondary=True)\n",
    "\n",
    "\n",
    "\n",
    "''''stack = GridSearchCV(\n",
    "       estimator=stack_gen, param_grid={},\n",
    "        cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "stack.fit(X, y)\n",
    "Stack score with lightgbm: 0.1195, \n",
    "y_pred_xboost=stack.predict(X_test)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted average of the best performing models was found which resulted in the best validation RMSE of 0.073."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">START Fit\n",
       "stack_gen\n",
       "elasticnet\n",
       "Lasso\n",
       "GradientBoosting\n",
       "xgboost\n",
       "lightgbm\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('START Fit')\n",
    "\n",
    "print('stack_gen')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "\n",
    "print('elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "\n",
    "print('Lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "\n",
    "##### fix weights\n",
    "\n",
    "print('GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "\n",
    "print('xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "\n",
    "print('lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">RMSLE score on train data:\n",
       "0.07318618629769486\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def blend_models_predict(X):\n",
    "    return ((((1/6) * elastic_model_full_data.predict(X)) + \n",
    "            ((1/6) * lasso_model_full_data.predict(X)) + \n",
    "            ((1/6) * gbr_model_full_data.predict(X)) + \n",
    "            ((1/6) * xgb_model_full_data.predict(X)) + \n",
    "            ((1/6) * lgb_model_full_data.predict(X)) + \n",
    "            ((1/6)* stack_gen_model.predict(np.array(X)))))\n",
    "  \n",
    "  \n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "blend=(rmsle(y, blend_models_predict(X)))\n",
    "print(blend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lasso regression</th>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Regression</th>\n",
       "      <td>0.2808</td>\n",
       "      <td>0.5067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elastic Net Regression</th>\n",
       "      <td>0.1284</td>\n",
       "      <td>0.1276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.1682</td>\n",
       "      <td>0.1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosted Classifier</th>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.1207</td>\n",
       "      <td>0.1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacked Models</th>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.1221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Avg Model</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.1194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\033[1m' + 'Regression Metrics for ML Models \\n')\n",
    "d2 = {'Validation RMSE': [0.1251, 0.2808, 0.1284, 0.1682, 0.1208, 0.1207, 0.1166, 0.1195, 0.0740], 'Test RMSE': [0.1256, 0.5067, 0.1276, 0.1734, 0.1287, 0.1273, 0.1229, 0.1221, 0.1194]}\n",
    "index2=['Lasso regression', 'Ridge Regression', 'Elastic Net Regression', 'Random Forest', 'Gradient Boosted Classifier', 'XGBoost', 'LightGBM', 'Stacked Models', 'Weighted Avg Model']                \n",
    "\n",
    "df2 = pd.DataFrame(data=d2,  index=index2)\n",
    "\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>SalePrice</th></tr></thead><tbody><tr><td>1461</td><td>122889.0</td></tr><tr><td>1462</td><td>159489.0</td></tr><tr><td>1463</td><td>184760.0</td></tr><tr><td>1464</td><td>198275.0</td></tr><tr><td>1465</td><td>192850.0</td></tr><tr><td>1466</td><td>171886.0</td></tr><tr><td>1467</td><td>180091.0</td></tr><tr><td>1468</td><td>165800.0</td></tr><tr><td>1469</td><td>188009.0</td></tr><tr><td>1470</td><td>122812.0</td></tr><tr><td>1471</td><td>190794.0</td></tr><tr><td>1472</td><td>97216.0</td></tr><tr><td>1473</td><td>94735.0</td></tr><tr><td>1474</td><td>149273.0</td></tr><tr><td>1475</td><td>113317.0</td></tr><tr><td>1476</td><td>372248.0</td></tr><tr><td>1477</td><td>254136.0</td></tr><tr><td>1478</td><td>285245.0</td></tr><tr><td>1479</td><td>277965.0</td></tr><tr><td>1480</td><td>464377.0</td></tr><tr><td>1481</td><td>337578.0</td></tr><tr><td>1482</td><td>209876.0</td></tr><tr><td>1483</td><td>174379.0</td></tr><tr><td>1484</td><td>165342.0</td></tr><tr><td>1485</td><td>183650.0</td></tr><tr><td>1486</td><td>194802.0</td></tr><tr><td>1487</td><td>349126.0</td></tr><tr><td>1488</td><td>238906.0</td></tr><tr><td>1489</td><td>201710.0</td></tr><tr><td>1490</td><td>236904.0</td></tr><tr><td>1491</td><td>194282.0</td></tr><tr><td>1492</td><td>91264.0</td></tr><tr><td>1493</td><td>187558.0</td></tr><tr><td>1494</td><td>292785.0</td></tr><tr><td>1495</td><td>290164.0</td></tr><tr><td>1496</td><td>236676.0</td></tr><tr><td>1497</td><td>181156.0</td></tr><tr><td>1498</td><td>163427.0</td></tr><tr><td>1499</td><td>161405.0</td></tr><tr><td>1500</td><td>156604.0</td></tr><tr><td>1501</td><td>178069.0</td></tr><tr><td>1502</td><td>145773.0</td></tr><tr><td>1503</td><td>292643.0</td></tr><tr><td>1504</td><td>243392.0</td></tr><tr><td>1505</td><td>225517.0</td></tr><tr><td>1506</td><td>186671.0</td></tr><tr><td>1507</td><td>249791.0</td></tr><tr><td>1508</td><td>203046.0</td></tr><tr><td>1509</td><td>159880.0</td></tr><tr><td>1510</td><td>146582.0</td></tr><tr><td>1511</td><td>146487.0</td></tr><tr><td>1512</td><td>176864.0</td></tr><tr><td>1513</td><td>151013.0</td></tr><tr><td>1514</td><td>161909.0</td></tr><tr><td>1515</td><td>187714.0</td></tr><tr><td>1516</td><td>150095.0</td></tr><tr><td>1517</td><td>168697.0</td></tr><tr><td>1518</td><td>130853.0</td></tr><tr><td>1519</td><td>217369.0</td></tr><tr><td>1520</td><td>131946.0</td></tr><tr><td>1521</td><td>133507.0</td></tr><tr><td>1522</td><td>173028.0</td></tr><tr><td>1523</td><td>113425.0</td></tr><tr><td>1524</td><td>126227.0</td></tr><tr><td>1525</td><td>123095.0</td></tr><tr><td>1526</td><td>115951.0</td></tr><tr><td>1527</td><td>104782.0</td></tr><tr><td>1528</td><td>136035.0</td></tr><tr><td>1529</td><td>152118.0</td></tr><tr><td>1530</td><td>184248.0</td></tr><tr><td>1531</td><td>105266.0</td></tr><tr><td>1532</td><td>98356.0</td></tr><tr><td>1533</td><td>147496.0</td></tr><tr><td>1534</td><td>123267.0</td></tr><tr><td>1535</td><td>147859.0</td></tr><tr><td>1536</td><td>111424.0</td></tr><tr><td>1537</td><td>64554.0</td></tr><tr><td>1538</td><td>155516.0</td></tr><tr><td>1539</td><td>198354.0</td></tr><tr><td>1540</td><td>91306.0</td></tr><tr><td>1541</td><td>140556.0</td></tr><tr><td>1542</td><td>139612.0</td></tr><tr><td>1543</td><td>196056.0</td></tr><tr><td>1544</td><td>85582.0</td></tr><tr><td>1545</td><td>113500.0</td></tr><tr><td>1546</td><td>133244.0</td></tr><tr><td>1547</td><td>133756.0</td></tr><tr><td>1548</td><td>138557.0</td></tr><tr><td>1549</td><td>115483.0</td></tr><tr><td>1550</td><td>139349.0</td></tr><tr><td>1551</td><td>110596.0</td></tr><tr><td>1552</td><td>135910.0</td></tr><tr><td>1553</td><td>156054.0</td></tr><tr><td>1554</td><td>118301.0</td></tr><tr><td>1555</td><td>162527.0</td></tr><tr><td>1556</td><td>83213.0</td></tr><tr><td>1557</td><td>104787.0</td></tr><tr><td>1558</td><td>94816.0</td></tr><tr><td>1559</td><td>72774.0</td></tr><tr><td>1560</td><td>133997.0</td></tr><tr><td>1561</td><td>136373.0</td></tr><tr><td>1562</td><td>129486.0</td></tr><tr><td>1563</td><td>122633.0</td></tr><tr><td>1564</td><td>155809.0</td></tr><tr><td>1565</td><td>146355.0</td></tr><tr><td>1566</td><td>243747.0</td></tr><tr><td>1567</td><td>73399.0</td></tr><tr><td>1568</td><td>230452.0</td></tr><tr><td>1569</td><td>134680.0</td></tr><tr><td>1570</td><td>137802.0</td></tr><tr><td>1571</td><td>115119.0</td></tr><tr><td>1572</td><td>142627.0</td></tr><tr><td>1573</td><td>234718.0</td></tr><tr><td>1574</td><td>116768.0</td></tr><tr><td>1575</td><td>220011.0</td></tr><tr><td>1576</td><td>250646.0</td></tr><tr><td>1577</td><td>188713.0</td></tr><tr><td>1578</td><td>146306.0</td></tr><tr><td>1579</td><td>143168.0</td></tr><tr><td>1580</td><td>191707.0</td></tr><tr><td>1581</td><td>157750.0</td></tr><tr><td>1582</td><td>128371.0</td></tr><tr><td>1583</td><td>314226.0</td></tr><tr><td>1584</td><td>223638.0</td></tr><tr><td>1585</td><td>139263.0</td></tr><tr><td>1586</td><td>64490.0</td></tr><tr><td>1587</td><td>98467.0</td></tr><tr><td>1588</td><td>152296.0</td></tr><tr><td>1589</td><td>100289.0</td></tr><tr><td>1590</td><td>133612.0</td></tr><tr><td>1591</td><td>94644.0</td></tr><tr><td>1592</td><td>118022.0</td></tr><tr><td>1593</td><td>130332.0</td></tr><tr><td>1594</td><td>143117.0</td></tr><tr><td>1595</td><td>110132.0</td></tr><tr><td>1596</td><td>234798.0</td></tr><tr><td>1597</td><td>185628.0</td></tr><tr><td>1598</td><td>218180.0</td></tr><tr><td>1599</td><td>164288.0</td></tr><tr><td>1600</td><td>164467.0</td></tr><tr><td>1601</td><td>60550.0</td></tr><tr><td>1602</td><td>116724.0</td></tr><tr><td>1603</td><td>79591.0</td></tr><tr><td>1604</td><td>266572.0</td></tr><tr><td>1605</td><td>242719.0</td></tr><tr><td>1606</td><td>173781.0</td></tr><tr><td>1607</td><td>168889.0</td></tr><tr><td>1608</td><td>218463.0</td></tr><tr><td>1609</td><td>187424.0</td></tr><tr><td>1610</td><td>152786.0</td></tr><tr><td>1611</td><td>138198.0</td></tr><tr><td>1612</td><td>166888.0</td></tr><tr><td>1613</td><td>166603.0</td></tr><tr><td>1614</td><td>130986.0</td></tr><tr><td>1615</td><td>91425.0</td></tr><tr><td>1616</td><td>73816.0</td></tr><tr><td>1617</td><td>93353.0</td></tr><tr><td>1618</td><td>125581.0</td></tr><tr><td>1619</td><td>141242.0</td></tr><tr><td>1620</td><td>161966.0</td></tr><tr><td>1621</td><td>135535.0</td></tr><tr><td>1622</td><td>137982.0</td></tr><tr><td>1623</td><td>280198.0</td></tr><tr><td>1624</td><td>206693.0</td></tr><tr><td>1625</td><td>119963.0</td></tr><tr><td>1626</td><td>164159.0</td></tr><tr><td>1627</td><td>188628.0</td></tr><tr><td>1628</td><td>281089.0</td></tr><tr><td>1629</td><td>174944.0</td></tr><tr><td>1630</td><td>349473.0</td></tr><tr><td>1631</td><td>222013.0</td></tr><tr><td>1632</td><td>240755.0</td></tr><tr><td>1633</td><td>169867.0</td></tr><tr><td>1634</td><td>187062.0</td></tr><tr><td>1635</td><td>175569.0</td></tr><tr><td>1636</td><td>154131.0</td></tr><tr><td>1637</td><td>201267.0</td></tr><tr><td>1638</td><td>185867.0</td></tr><tr><td>1639</td><td>185121.0</td></tr><tr><td>1640</td><td>259395.0</td></tr><tr><td>1641</td><td>178437.0</td></tr><tr><td>1642</td><td>235614.0</td></tr><tr><td>1643</td><td>217220.0</td></tr><tr><td>1644</td><td>233125.0</td></tr><tr><td>1645</td><td>201629.0</td></tr><tr><td>1646</td><td>161003.0</td></tr><tr><td>1647</td><td>156355.0</td></tr><tr><td>1648</td><td>132102.0</td></tr><tr><td>1649</td><td>140395.0</td></tr><tr><td>1650</td><td>116064.0</td></tr><tr><td>1651</td><td>123173.0</td></tr><tr><td>1652</td><td>96579.0</td></tr><tr><td>1653</td><td>100137.0</td></tr><tr><td>1654</td><td>144510.0</td></tr><tr><td>1655</td><td>143274.0</td></tr><tr><td>1656</td><td>143509.0</td></tr><tr><td>1657</td><td>147871.0</td></tr><tr><td>1658</td><td>144997.0</td></tr><tr><td>1659</td><td>120910.0</td></tr><tr><td>1660</td><td>154858.0</td></tr><tr><td>1661</td><td>433451.0</td></tr><tr><td>1662</td><td>409157.0</td></tr><tr><td>1663</td><td>379785.0</td></tr><tr><td>1664</td><td>422243.0</td></tr><tr><td>1665</td><td>315505.0</td></tr><tr><td>1666</td><td>324487.0</td></tr><tr><td>1667</td><td>372105.0</td></tr><tr><td>1668</td><td>339890.0</td></tr><tr><td>1669</td><td>301684.0</td></tr><tr><td>1670</td><td>349424.0</td></tr><tr><td>1671</td><td>259441.0</td></tr><tr><td>1672</td><td>441812.0</td></tr><tr><td>1673</td><td>292568.0</td></tr><tr><td>1674</td><td>242409.0</td></tr><tr><td>1675</td><td>188494.0</td></tr><tr><td>1676</td><td>190249.0</td></tr><tr><td>1677</td><td>219321.0</td></tr><tr><td>1678</td><td>441129.0</td></tr><tr><td>1679</td><td>396112.0</td></tr><tr><td>1680</td><td>339616.0</td></tr><tr><td>1681</td><td>248144.0</td></tr><tr><td>1682</td><td>303919.0</td></tr><tr><td>1683</td><td>177313.0</td></tr><tr><td>1684</td><td>172306.0</td></tr><tr><td>1685</td><td>171240.0</td></tr><tr><td>1686</td><td>168188.0</td></tr><tr><td>1687</td><td>167389.0</td></tr><tr><td>1688</td><td>188743.0</td></tr><tr><td>1689</td><td>197778.0</td></tr><tr><td>1690</td><td>195592.0</td></tr><tr><td>1691</td><td>183010.0</td></tr><tr><td>1692</td><td>268991.0</td></tr><tr><td>1693</td><td>166952.0</td></tr><tr><td>1694</td><td>182139.0</td></tr><tr><td>1695</td><td>166432.0</td></tr><tr><td>1696</td><td>270126.0</td></tr><tr><td>1697</td><td>173737.0</td></tr><tr><td>1698</td><td>351294.0</td></tr><tr><td>1699</td><td>319383.0</td></tr><tr><td>1700</td><td>244992.0</td></tr><tr><td>1701</td><td>268274.0</td></tr><tr><td>1702</td><td>245066.0</td></tr><tr><td>1703</td><td>249008.0</td></tr><tr><td>1704</td><td>280376.0</td></tr><tr><td>1705</td><td>235698.0</td></tr><tr><td>1706</td><td>412333.0</td></tr><tr><td>1707</td><td>215954.0</td></tr><tr><td>1708</td><td>206691.0</td></tr><tr><td>1709</td><td>273684.0</td></tr><tr><td>1710</td><td>221150.0</td></tr><tr><td>1711</td><td>267227.0</td></tr><tr><td>1712</td><td>259908.0</td></tr><tr><td>1713</td><td>274783.0</td></tr><tr><td>1714</td><td>222417.0</td></tr><tr><td>1715</td><td>206190.0</td></tr><tr><td>1716</td><td>183953.0</td></tr><tr><td>1717</td><td>175047.0</td></tr><tr><td>1718</td><td>137634.0</td></tr><tr><td>1719</td><td>202126.0</td></tr><tr><td>1720</td><td>243145.0</td></tr><tr><td>1721</td><td>169281.0</td></tr><tr><td>1722</td><td>127554.0</td></tr><tr><td>1723</td><td>157142.0</td></tr><tr><td>1724</td><td>209540.0</td></tr><tr><td>1725</td><td>239538.0</td></tr><tr><td>1726</td><td>186094.0</td></tr><tr><td>1727</td><td>155607.0</td></tr><tr><td>1728</td><td>178497.0</td></tr><tr><td>1729</td><td>167009.0</td></tr><tr><td>1730</td><td>159561.0</td></tr><tr><td>1731</td><td>122758.0</td></tr><tr><td>1732</td><td>126797.0</td></tr><tr><td>1733</td><td>116314.0</td></tr><tr><td>1734</td><td>123237.0</td></tr><tr><td>1735</td><td>128692.0</td></tr><tr><td>1736</td><td>114457.0</td></tr><tr><td>1737</td><td>310017.0</td></tr><tr><td>1738</td><td>241025.0</td></tr><tr><td>1739</td><td>257475.0</td></tr><tr><td>1740</td><td>218018.0</td></tr><tr><td>1741</td><td>188511.0</td></tr><tr><td>1742</td><td>173692.0</td></tr><tr><td>1743</td><td>175239.0</td></tr><tr><td>1744</td><td>307047.0</td></tr><tr><td>1745</td><td>217493.0</td></tr><tr><td>1746</td><td>181236.0</td></tr><tr><td>1747</td><td>213694.0</td></tr><tr><td>1748</td><td>226086.0</td></tr><tr><td>1749</td><td>151363.0</td></tr><tr><td>1750</td><td>126668.0</td></tr><tr><td>1751</td><td>239801.0</td></tr><tr><td>1752</td><td>121300.0</td></tr><tr><td>1753</td><td>148049.0</td></tr><tr><td>1754</td><td>190815.0</td></tr><tr><td>1755</td><td>169539.0</td></tr><tr><td>1756</td><td>131911.0</td></tr><tr><td>1757</td><td>122026.0</td></tr><tr><td>1758</td><td>143474.0</td></tr><tr><td>1759</td><td>161416.0</td></tr><tr><td>1760</td><td>165624.0</td></tr><tr><td>1761</td><td>143763.0</td></tr><tr><td>1762</td><td>181003.0</td></tr><tr><td>1763</td><td>174894.0</td></tr><tr><td>1764</td><td>116215.0</td></tr><tr><td>1765</td><td>158510.0</td></tr><tr><td>1766</td><td>184186.0</td></tr><tr><td>1767</td><td>230631.0</td></tr><tr><td>1768</td><td>144604.0</td></tr><tr><td>1769</td><td>172826.0</td></tr><tr><td>1770</td><td>152578.0</td></tr><tr><td>1771</td><td>126235.0</td></tr><tr><td>1772</td><td>135937.0</td></tr><tr><td>1773</td><td>133702.0</td></tr><tr><td>1774</td><td>137321.0</td></tr><tr><td>1775</td><td>139976.0</td></tr><tr><td>1776</td><td>129489.0</td></tr><tr><td>1777</td><td>111148.0</td></tr><tr><td>1778</td><td>139862.0</td></tr><tr><td>1779</td><td>116732.0</td></tr><tr><td>1780</td><td>176864.0</td></tr><tr><td>1781</td><td>127474.0</td></tr><tr><td>1782</td><td>86563.0</td></tr><tr><td>1783</td><td>143652.0</td></tr><tr><td>1784</td><td>109513.0</td></tr><tr><td>1785</td><td>120494.0</td></tr><tr><td>1786</td><td>131069.0</td></tr><tr><td>1787</td><td>163573.0</td></tr><tr><td>1788</td><td>55267.0</td></tr><tr><td>1789</td><td>95024.0</td></tr><tr><td>1790</td><td>74039.0</td></tr><tr><td>1791</td><td>175062.0</td></tr><tr><td>1792</td><td>159848.0</td></tr><tr><td>1793</td><td>130567.0</td></tr><tr><td>1794</td><td>150172.0</td></tr><tr><td>1795</td><td>131545.0</td></tr><tr><td>1796</td><td>120752.0</td></tr><tr><td>1797</td><td>115301.0</td></tr><tr><td>1798</td><td>123257.0</td></tr><tr><td>1799</td><td>110425.0</td></tr><tr><td>1800</td><td>130110.0</td></tr><tr><td>1801</td><td>132306.0</td></tr><tr><td>1802</td><td>127747.0</td></tr><tr><td>1803</td><td>146975.0</td></tr><tr><td>1804</td><td>137526.0</td></tr><tr><td>1805</td><td>136560.0</td></tr><tr><td>1806</td><td>122733.0</td></tr><tr><td>1807</td><td>129132.0</td></tr><tr><td>1808</td><td>121753.0</td></tr><tr><td>1809</td><td>124963.0</td></tr><tr><td>1810</td><td>144934.0</td></tr><tr><td>1811</td><td>95253.0</td></tr><tr><td>1812</td><td>94997.0</td></tr><tr><td>1813</td><td>122712.0</td></tr><tr><td>1814</td><td>96209.0</td></tr><tr><td>1815</td><td>52469.0</td></tr><tr><td>1816</td><td>98317.0</td></tr><tr><td>1817</td><td>105268.0</td></tr><tr><td>1818</td><td>147268.0</td></tr><tr><td>1819</td><td>127520.0</td></tr><tr><td>1820</td><td>56860.0</td></tr><tr><td>1821</td><td>109462.0</td></tr><tr><td>1822</td><td>146704.0</td></tr><tr><td>1823</td><td>47127.0</td></tr><tr><td>1824</td><td>131474.0</td></tr><tr><td>1825</td><td>135459.0</td></tr><tr><td>1826</td><td>103791.0</td></tr><tr><td>1827</td><td>100259.0</td></tr><tr><td>1828</td><td>139685.0</td></tr><tr><td>1829</td><td>117165.0</td></tr><tr><td>1830</td><td>145658.0</td></tr><tr><td>1831</td><td>141892.0</td></tr><tr><td>1832</td><td>75008.0</td></tr><tr><td>1833</td><td>141627.0</td></tr><tr><td>1834</td><td>118907.0</td></tr><tr><td>1835</td><td>125504.0</td></tr><tr><td>1836</td><td>124131.0</td></tr><tr><td>1837</td><td>89992.0</td></tr><tr><td>1838</td><td>128168.0</td></tr><tr><td>1839</td><td>100250.0</td></tr><tr><td>1840</td><td>154780.0</td></tr><tr><td>1841</td><td>143952.0</td></tr><tr><td>1842</td><td>85771.0</td></tr><tr><td>1843</td><td>128934.0</td></tr><tr><td>1844</td><td>136441.0</td></tr><tr><td>1845</td><td>140479.0</td></tr><tr><td>1846</td><td>154193.0</td></tr><tr><td>1847</td><td>163534.0</td></tr><tr><td>1848</td><td>53791.0</td></tr><tr><td>1849</td><td>109279.0</td></tr><tr><td>1850</td><td>117294.0</td></tr><tr><td>1851</td><td>148549.0</td></tr><tr><td>1852</td><td>124037.0</td></tr><tr><td>1853</td><td>128311.0</td></tr><tr><td>1854</td><td>169037.0</td></tr><tr><td>1855</td><td>143880.0</td></tr><tr><td>1856</td><td>216485.0</td></tr><tr><td>1857</td><td>146178.0</td></tr><tr><td>1858</td><td>138874.0</td></tr><tr><td>1859</td><td>119594.0</td></tr><tr><td>1860</td><td>145621.0</td></tr><tr><td>1861</td><td>118677.0</td></tr><tr><td>1862</td><td>318678.0</td></tr><tr><td>1863</td><td>299252.0</td></tr><tr><td>1864</td><td>299328.0</td></tr><tr><td>1865</td><td>332040.0</td></tr><tr><td>1866</td><td>336797.0</td></tr><tr><td>1867</td><td>232751.0</td></tr><tr><td>1868</td><td>291906.0</td></tr><tr><td>1869</td><td>204859.0</td></tr><tr><td>1870</td><td>230636.0</td></tr><tr><td>1871</td><td>253332.0</td></tr><tr><td>1872</td><td>170919.0</td></tr><tr><td>1873</td><td>237422.0</td></tr><tr><td>1874</td><td>145876.0</td></tr><tr><td>1875</td><td>197427.0</td></tr><tr><td>1876</td><td>196047.0</td></tr><tr><td>1877</td><td>207132.0</td></tr><tr><td>1878</td><td>206158.0</td></tr><tr><td>1879</td><td>127820.0</td></tr><tr><td>1880</td><td>133180.0</td></tr><tr><td>1881</td><td>244814.0</td></tr><tr><td>1882</td><td>248969.0</td></tr><tr><td>1883</td><td>187817.0</td></tr><tr><td>1884</td><td>205900.0</td></tr><tr><td>1885</td><td>240590.0</td></tr><tr><td>1886</td><td>291215.0</td></tr><tr><td>1887</td><td>208872.0</td></tr><tr><td>1888</td><td>275779.0</td></tr><tr><td>1889</td><td>161598.0</td></tr><tr><td>1890</td><td>120876.0</td></tr><tr><td>1891</td><td>131165.0</td></tr><tr><td>1892</td><td>99164.0</td></tr><tr><td>1893</td><td>128604.0</td></tr><tr><td>1894</td><td>123038.0</td></tr><tr><td>1895</td><td>131452.0</td></tr><tr><td>1896</td><td>123713.0</td></tr><tr><td>1897</td><td>115779.0</td></tr><tr><td>1898</td><td>111256.0</td></tr><tr><td>1899</td><td>156867.0</td></tr><tr><td>1900</td><td>152384.0</td></tr><tr><td>1901</td><td>171601.0</td></tr><tr><td>1902</td><td>146383.0</td></tr><tr><td>1903</td><td>218887.0</td></tr><tr><td>1904</td><td>145575.0</td></tr><tr><td>1905</td><td>202065.0</td></tr><tr><td>1906</td><td>165025.0</td></tr><tr><td>1907</td><td>196914.0</td></tr><tr><td>1908</td><td>109386.0</td></tr><tr><td>1909</td><td>131455.0</td></tr><tr><td>1910</td><td>118278.0</td></tr><tr><td>1911</td><td>215621.0</td></tr><tr><td>1912</td><td>314442.0</td></tr><tr><td>1913</td><td>140589.0</td></tr><tr><td>1914</td><td>60697.0</td></tr><tr><td>1915</td><td>312633.0</td></tr><tr><td>1916</td><td>63264.0</td></tr><tr><td>1917</td><td>245697.0</td></tr><tr><td>1918</td><td>141653.0</td></tr><tr><td>1919</td><td>171654.0</td></tr><tr><td>1920</td><td>153158.0</td></tr><tr><td>1921</td><td>408266.0</td></tr><tr><td>1922</td><td>318080.0</td></tr><tr><td>1923</td><td>225946.0</td></tr><tr><td>1924</td><td>227553.0</td></tr><tr><td>1925</td><td>192919.0</td></tr><tr><td>1926</td><td>394936.0</td></tr><tr><td>1927</td><td>130584.0</td></tr><tr><td>1928</td><td>155983.0</td></tr><tr><td>1929</td><td>126094.0</td></tr><tr><td>1930</td><td>129242.0</td></tr><tr><td>1931</td><td>143619.0</td></tr><tr><td>1932</td><td>138871.0</td></tr><tr><td>1933</td><td>179886.0</td></tr><tr><td>1934</td><td>180495.0</td></tr><tr><td>1935</td><td>169684.0</td></tr><tr><td>1936</td><td>191977.0</td></tr><tr><td>1937</td><td>184003.0</td></tr><tr><td>1938</td><td>171496.0</td></tr><tr><td>1939</td><td>241053.0</td></tr><tr><td>1940</td><td>190464.0</td></tr><tr><td>1941</td><td>172630.0</td></tr><tr><td>1942</td><td>184319.0</td></tr><tr><td>1943</td><td>213637.0</td></tr><tr><td>1944</td><td>373876.0</td></tr><tr><td>1945</td><td>411941.0</td></tr><tr><td>1946</td><td>134633.0</td></tr><tr><td>1947</td><td>311177.0</td></tr><tr><td>1948</td><td>173437.0</td></tr><tr><td>1949</td><td>252597.0</td></tr><tr><td>1950</td><td>187780.0</td></tr><tr><td>1951</td><td>237376.0</td></tr><tr><td>1952</td><td>210974.0</td></tr><tr><td>1953</td><td>169129.0</td></tr><tr><td>1954</td><td>182872.0</td></tr><tr><td>1955</td><td>134096.0</td></tr><tr><td>1956</td><td>308922.0</td></tr><tr><td>1957</td><td>153103.0</td></tr><tr><td>1958</td><td>304210.0</td></tr><tr><td>1959</td><td>142567.0</td></tr><tr><td>1960</td><td>117744.0</td></tr><tr><td>1961</td><td>123325.0</td></tr><tr><td>1962</td><td>95668.0</td></tr><tr><td>1963</td><td>104199.0</td></tr><tr><td>1964</td><td>111290.0</td></tr><tr><td>1965</td><td>144429.0</td></tr><tr><td>1966</td><td>137580.0</td></tr><tr><td>1967</td><td>287549.0</td></tr><tr><td>1968</td><td>430966.0</td></tr><tr><td>1969</td><td>405828.0</td></tr><tr><td>1970</td><td>438742.0</td></tr><tr><td>1971</td><td>450503.0</td></tr><tr><td>1972</td><td>386950.0</td></tr><tr><td>1973</td><td>264151.0</td></tr><tr><td>1974</td><td>328940.0</td></tr><tr><td>1975</td><td>453829.0</td></tr><tr><td>1976</td><td>271488.0</td></tr><tr><td>1977</td><td>351352.0</td></tr><tr><td>1978</td><td>359938.0</td></tr><tr><td>1979</td><td>323601.0</td></tr><tr><td>1980</td><td>198894.0</td></tr><tr><td>1981</td><td>333296.0</td></tr><tr><td>1982</td><td>218612.0</td></tr><tr><td>1983</td><td>203234.0</td></tr><tr><td>1984</td><td>169745.0</td></tr><tr><td>1985</td><td>222162.0</td></tr><tr><td>1986</td><td>209384.0</td></tr><tr><td>1987</td><td>180519.0</td></tr><tr><td>1988</td><td>175582.0</td></tr><tr><td>1989</td><td>194500.0</td></tr><tr><td>1990</td><td>212088.0</td></tr><tr><td>1991</td><td>235960.0</td></tr><tr><td>1992</td><td>216930.0</td></tr><tr><td>1993</td><td>167579.0</td></tr><tr><td>1994</td><td>229301.0</td></tr><tr><td>1995</td><td>185328.0</td></tr><tr><td>1996</td><td>248011.0</td></tr><tr><td>1997</td><td>319968.0</td></tr><tr><td>1998</td><td>329102.0</td></tr><tr><td>1999</td><td>265021.0</td></tr><tr><td>2000</td><td>322246.0</td></tr><tr><td>2001</td><td>273580.0</td></tr><tr><td>2002</td><td>241294.0</td></tr><tr><td>2003</td><td>264805.0</td></tr><tr><td>2004</td><td>272051.0</td></tr><tr><td>2005</td><td>219003.0</td></tr><tr><td>2006</td><td>226816.0</td></tr><tr><td>2007</td><td>249624.0</td></tr><tr><td>2008</td><td>216435.0</td></tr><tr><td>2009</td><td>197159.0</td></tr><tr><td>2010</td><td>195068.0</td></tr><tr><td>2011</td><td>144462.0</td></tr><tr><td>2012</td><td>174772.0</td></tr><tr><td>2013</td><td>178809.0</td></tr><tr><td>2014</td><td>185449.0</td></tr><tr><td>2015</td><td>207450.0</td></tr><tr><td>2016</td><td>192421.0</td></tr><tr><td>2017</td><td>196172.0</td></tr><tr><td>2018</td><td>120893.0</td></tr><tr><td>2019</td><td>136123.0</td></tr><tr><td>2020</td><td>107400.0</td></tr><tr><td>2021</td><td>104430.0</td></tr><tr><td>2022</td><td>184355.0</td></tr><tr><td>2023</td><td>143786.0</td></tr><tr><td>2024</td><td>257848.0</td></tr><tr><td>2025</td><td>356677.0</td></tr><tr><td>2026</td><td>174078.0</td></tr><tr><td>2027</td><td>158208.0</td></tr><tr><td>2028</td><td>148868.0</td></tr><tr><td>2029</td><td>170119.0</td></tr><tr><td>2030</td><td>272570.0</td></tr><tr><td>2031</td><td>237814.0</td></tr><tr><td>2032</td><td>253721.0</td></tr><tr><td>2033</td><td>262015.0</td></tr><tr><td>2034</td><td>170221.0</td></tr><tr><td>2035</td><td>218812.0</td></tr><tr><td>2036</td><td>199053.0</td></tr><tr><td>2037</td><td>204908.0</td></tr><tr><td>2038</td><td>310340.0</td></tr><tr><td>2039</td><td>223634.0</td></tr><tr><td>2040</td><td>279050.0</td></tr><tr><td>2041</td><td>300963.0</td></tr><tr><td>2042</td><td>207541.0</td></tr><tr><td>2043</td><td>177691.0</td></tr><tr><td>2044</td><td>168822.0</td></tr><tr><td>2045</td><td>213329.0</td></tr><tr><td>2046</td><td>143219.0</td></tr><tr><td>2047</td><td>145886.0</td></tr><tr><td>2048</td><td>140886.0</td></tr><tr><td>2049</td><td>139605.0</td></tr><tr><td>2050</td><td>174863.0</td></tr><tr><td>2051</td><td>104413.0</td></tr><tr><td>2052</td><td>128173.0</td></tr><tr><td>2053</td><td>146807.0</td></tr><tr><td>2054</td><td>86605.0</td></tr><tr><td>2055</td><td>155858.0</td></tr><tr><td>2056</td><td>144529.0</td></tr><tr><td>2057</td><td>115408.0</td></tr><tr><td>2058</td><td>217337.0</td></tr><tr><td>2059</td><td>132534.0</td></tr><tr><td>2060</td><td>173969.0</td></tr><tr><td>2061</td><td>174264.0</td></tr><tr><td>2062</td><td>131215.0</td></tr><tr><td>2063</td><td>118259.0</td></tr><tr><td>2064</td><td>143530.0</td></tr><tr><td>2065</td><td>120623.0</td></tr><tr><td>2066</td><td>161211.0</td></tr><tr><td>2067</td><td>119506.0</td></tr><tr><td>2068</td><td>138991.0</td></tr><tr><td>2069</td><td>88725.0</td></tr><tr><td>2070</td><td>118855.0</td></tr><tr><td>2071</td><td>93995.0</td></tr><tr><td>2072</td><td>134905.0</td></tr><tr><td>2073</td><td>137348.0</td></tr><tr><td>2074</td><td>183933.0</td></tr><tr><td>2075</td><td>147415.0</td></tr><tr><td>2076</td><td>120295.0</td></tr><tr><td>2077</td><td>158505.0</td></tr><tr><td>2078</td><td>129358.0</td></tr><tr><td>2079</td><td>132747.0</td></tr><tr><td>2080</td><td>119609.0</td></tr><tr><td>2081</td><td>127067.0</td></tr><tr><td>2082</td><td>122731.0</td></tr><tr><td>2083</td><td>146251.0</td></tr><tr><td>2084</td><td>113140.0</td></tr><tr><td>2085</td><td>118707.0</td></tr><tr><td>2086</td><td>112055.0</td></tr><tr><td>2087</td><td>119951.0</td></tr><tr><td>2088</td><td>96668.0</td></tr><tr><td>2089</td><td>73791.0</td></tr><tr><td>2090</td><td>126414.0</td></tr><tr><td>2091</td><td>119320.0</td></tr><tr><td>2092</td><td>124256.0</td></tr><tr><td>2093</td><td>114434.0</td></tr><tr><td>2094</td><td>110414.0</td></tr><tr><td>2095</td><td>148497.0</td></tr><tr><td>2096</td><td>79935.0</td></tr><tr><td>2097</td><td>98806.0</td></tr><tr><td>2098</td><td>144957.0</td></tr><tr><td>2099</td><td>54992.0</td></tr><tr><td>2100</td><td>79904.0</td></tr><tr><td>2101</td><td>118955.0</td></tr><tr><td>2102</td><td>124576.0</td></tr><tr><td>2103</td><td>105171.0</td></tr><tr><td>2104</td><td>141066.0</td></tr><tr><td>2105</td><td>129107.0</td></tr><tr><td>2106</td><td>66687.0</td></tr><tr><td>2107</td><td>200158.0</td></tr><tr><td>2108</td><td>117944.0</td></tr><tr><td>2109</td><td>112661.0</td></tr><tr><td>2110</td><td>128469.0</td></tr><tr><td>2111</td><td>135865.0</td></tr><tr><td>2112</td><td>142371.0</td></tr><tr><td>2113</td><td>119349.0</td></tr><tr><td>2114</td><td>119767.0</td></tr><tr><td>2115</td><td>162253.0</td></tr><tr><td>2116</td><td>115066.0</td></tr><tr><td>2117</td><td>151549.0</td></tr><tr><td>2118</td><td>126985.0</td></tr><tr><td>2119</td><td>112999.0</td></tr><tr><td>2120</td><td>112829.0</td></tr><tr><td>2121</td><td>109295.0</td></tr><tr><td>2122</td><td>116514.0</td></tr><tr><td>2123</td><td>81424.0</td></tr><tr><td>2124</td><td>158814.0</td></tr><tr><td>2125</td><td>126834.0</td></tr><tr><td>2126</td><td>147312.0</td></tr><tr><td>2127</td><td>156598.0</td></tr><tr><td>2128</td><td>124426.0</td></tr><tr><td>2129</td><td>94076.0</td></tr><tr><td>2130</td><td>140205.0</td></tr><tr><td>2131</td><td>131989.0</td></tr><tr><td>2132</td><td>117045.0</td></tr><tr><td>2133</td><td>123892.0</td></tr><tr><td>2134</td><td>124449.0</td></tr><tr><td>2135</td><td>94331.0</td></tr><tr><td>2136</td><td>79728.0</td></tr><tr><td>2137</td><td>108823.0</td></tr><tr><td>2138</td><td>130141.0</td></tr><tr><td>2139</td><td>151506.0</td></tr><tr><td>2140</td><td>137392.0</td></tr><tr><td>2141</td><td>154028.0</td></tr><tr><td>2142</td><td>126235.0</td></tr><tr><td>2143</td><td>147774.0</td></tr><tr><td>2144</td><td>120031.0</td></tr><tr><td>2145</td><td>136597.0</td></tr><tr><td>2146</td><td>169257.0</td></tr><tr><td>2147</td><td>139923.0</td></tr><tr><td>2148</td><td>135140.0</td></tr><tr><td>2149</td><td>148052.0</td></tr><tr><td>2150</td><td>233924.0</td></tr><tr><td>2151</td><td>111478.0</td></tr><tr><td>2152</td><td>166261.0</td></tr><tr><td>2153</td><td>165704.0</td></tr><tr><td>2154</td><td>105988.0</td></tr><tr><td>2155</td><td>141201.0</td></tr><tr><td>2156</td><td>269569.0</td></tr><tr><td>2157</td><td>231689.0</td></tr><tr><td>2158</td><td>239780.0</td></tr><tr><td>2159</td><td>206580.0</td></tr><tr><td>2160</td><td>173747.0</td></tr><tr><td>2161</td><td>235758.0</td></tr><tr><td>2162</td><td>390225.0</td></tr><tr><td>2163</td><td>338008.0</td></tr><tr><td>2164</td><td>249068.0</td></tr><tr><td>2165</td><td>202953.0</td></tr><tr><td>2166</td><td>153584.0</td></tr><tr><td>2167</td><td>219037.0</td></tr><tr><td>2168</td><td>194862.0</td></tr><tr><td>2169</td><td>196122.0</td></tr><tr><td>2170</td><td>218880.0</td></tr><tr><td>2171</td><td>152573.0</td></tr><tr><td>2172</td><td>131577.0</td></tr><tr><td>2173</td><td>167772.0</td></tr><tr><td>2174</td><td>223079.0</td></tr><tr><td>2175</td><td>297444.0</td></tr><tr><td>2176</td><td>322820.0</td></tr><tr><td>2177</td><td>235391.0</td></tr><tr><td>2178</td><td>211850.0</td></tr><tr><td>2179</td><td>139876.0</td></tr><tr><td>2180</td><td>227012.0</td></tr><tr><td>2181</td><td>190675.0</td></tr><tr><td>2182</td><td>223200.0</td></tr><tr><td>2183</td><td>190133.0</td></tr><tr><td>2184</td><td>121288.0</td></tr><tr><td>2185</td><td>119902.0</td></tr><tr><td>2186</td><td>144596.0</td></tr><tr><td>2187</td><td>150972.0</td></tr><tr><td>2188</td><td>152577.0</td></tr><tr><td>2189</td><td>288337.0</td></tr><tr><td>2190</td><td>82837.0</td></tr><tr><td>2191</td><td>85068.0</td></tr><tr><td>2192</td><td>91560.0</td></tr><tr><td>2193</td><td>104437.0</td></tr><tr><td>2194</td><td>101295.0</td></tr><tr><td>2195</td><td>106196.0</td></tr><tr><td>2196</td><td>96349.0</td></tr><tr><td>2197</td><td>114110.0</td></tr><tr><td>2198</td><td>159144.0</td></tr><tr><td>2199</td><td>184241.0</td></tr><tr><td>2200</td><td>149944.0</td></tr><tr><td>2201</td><td>146840.0</td></tr><tr><td>2202</td><td>215834.0</td></tr><tr><td>2203</td><td>139516.0</td></tr><tr><td>2204</td><td>180120.0</td></tr><tr><td>2205</td><td>114928.0</td></tr><tr><td>2206</td><td>145876.0</td></tr><tr><td>2207</td><td>218025.0</td></tr><tr><td>2208</td><td>259263.0</td></tr><tr><td>2209</td><td>243985.0</td></tr><tr><td>2210</td><td>117856.0</td></tr><tr><td>2211</td><td>116021.0</td></tr><tr><td>2212</td><td>119471.0</td></tr><tr><td>2213</td><td>103798.0</td></tr><tr><td>2214</td><td>134872.0</td></tr><tr><td>2215</td><td>97363.0</td></tr><tr><td>2216</td><td>144497.0</td></tr><tr><td>2217</td><td>64905.0</td></tr><tr><td>2218</td><td>74261.0</td></tr><tr><td>2219</td><td>71334.0</td></tr><tr><td>2220</td><td>68401.0</td></tr><tr><td>2221</td><td>313226.0</td></tr><tr><td>2222</td><td>277380.0</td></tr><tr><td>2223</td><td>298763.0</td></tr><tr><td>2224</td><td>212567.0</td></tr><tr><td>2225</td><td>123415.0</td></tr><tr><td>2226</td><td>185795.0</td></tr><tr><td>2227</td><td>200148.0</td></tr><tr><td>2228</td><td>267877.0</td></tr><tr><td>2229</td><td>247179.0</td></tr><tr><td>2230</td><td>152127.0</td></tr><tr><td>2231</td><td>214454.0</td></tr><tr><td>2232</td><td>181174.0</td></tr><tr><td>2233</td><td>175145.0</td></tr><tr><td>2234</td><td>240714.0</td></tr><tr><td>2235</td><td>218558.0</td></tr><tr><td>2236</td><td>255750.0</td></tr><tr><td>2237</td><td>316005.0</td></tr><tr><td>2238</td><td>201928.0</td></tr><tr><td>2239</td><td>109885.0</td></tr><tr><td>2240</td><td>159172.0</td></tr><tr><td>2241</td><td>154159.0</td></tr><tr><td>2242</td><td>125268.0</td></tr><tr><td>2243</td><td>127760.0</td></tr><tr><td>2244</td><td>103917.0</td></tr><tr><td>2245</td><td>100589.0</td></tr><tr><td>2246</td><td>139360.0</td></tr><tr><td>2247</td><td>116502.0</td></tr><tr><td>2248</td><td>125778.0</td></tr><tr><td>2249</td><td>120837.0</td></tr><tr><td>2250</td><td>131153.0</td></tr><tr><td>2251</td><td>100344.0</td></tr><tr><td>2252</td><td>172183.0</td></tr><tr><td>2253</td><td>151974.0</td></tr><tr><td>2254</td><td>177295.0</td></tr><tr><td>2255</td><td>185173.0</td></tr><tr><td>2256</td><td>180009.0</td></tr><tr><td>2257</td><td>210005.0</td></tr><tr><td>2258</td><td>162220.0</td></tr><tr><td>2259</td><td>179462.0</td></tr><tr><td>2260</td><td>157816.0</td></tr><tr><td>2261</td><td>191717.0</td></tr><tr><td>2262</td><td>203180.0</td></tr><tr><td>2263</td><td>386188.0</td></tr><tr><td>2264</td><td>438248.0</td></tr><tr><td>2265</td><td>168912.0</td></tr><tr><td>2266</td><td>305252.0</td></tr><tr><td>2267</td><td>358312.0</td></tr><tr><td>2268</td><td>411559.0</td></tr><tr><td>2269</td><td>151538.0</td></tr><tr><td>2270</td><td>197150.0</td></tr><tr><td>2271</td><td>214209.0</td></tr><tr><td>2272</td><td>192279.0</td></tr><tr><td>2273</td><td>164600.0</td></tr><tr><td>2274</td><td>185054.0</td></tr><tr><td>2275</td><td>157185.0</td></tr><tr><td>2276</td><td>193288.0</td></tr><tr><td>2277</td><td>180915.0</td></tr><tr><td>2278</td><td>147868.0</td></tr><tr><td>2279</td><td>125739.0</td></tr><tr><td>2280</td><td>121161.0</td></tr><tr><td>2281</td><td>169815.0</td></tr><tr><td>2282</td><td>184700.0</td></tr><tr><td>2283</td><td>104569.0</td></tr><tr><td>2284</td><td>112480.0</td></tr><tr><td>2285</td><td>138225.0</td></tr><tr><td>2286</td><td>124328.0</td></tr><tr><td>2287</td><td>351627.0</td></tr><tr><td>2288</td><td>270516.0</td></tr><tr><td>2289</td><td>381362.0</td></tr><tr><td>2290</td><td>436504.0</td></tr><tr><td>2291</td><td>330497.0</td></tr><tr><td>2292</td><td>466051.0</td></tr><tr><td>2293</td><td>441893.0</td></tr><tr><td>2294</td><td>429321.0</td></tr><tr><td>2295</td><td>447316.0</td></tr><tr><td>2296</td><td>279323.0</td></tr><tr><td>2297</td><td>330474.0</td></tr><tr><td>2298</td><td>340249.0</td></tr><tr><td>2299</td><td>359872.0</td></tr><tr><td>2300</td><td>344856.0</td></tr><tr><td>2301</td><td>298107.0</td></tr><tr><td>2302</td><td>251202.0</td></tr><tr><td>2303</td><td>244280.0</td></tr><tr><td>2304</td><td>242344.0</td></tr><tr><td>2305</td><td>186272.0</td></tr><tr><td>2306</td><td>184274.0</td></tr><tr><td>2307</td><td>193851.0</td></tr><tr><td>2308</td><td>225697.0</td></tr><tr><td>2309</td><td>285574.0</td></tr><tr><td>2310</td><td>208118.0</td></tr><tr><td>2311</td><td>194148.0</td></tr><tr><td>2312</td><td>175284.0</td></tr><tr><td>2313</td><td>173896.0</td></tr><tr><td>2314</td><td>166827.0</td></tr><tr><td>2315</td><td>177069.0</td></tr><tr><td>2316</td><td>191744.0</td></tr><tr><td>2317</td><td>187768.0</td></tr><tr><td>2318</td><td>177096.0</td></tr><tr><td>2319</td><td>179912.0</td></tr><tr><td>2320</td><td>186537.0</td></tr><tr><td>2321</td><td>236714.0</td></tr><tr><td>2322</td><td>175338.0</td></tr><tr><td>2323</td><td>189167.0</td></tr><tr><td>2324</td><td>175161.0</td></tr><tr><td>2325</td><td>214046.0</td></tr><tr><td>2326</td><td>171335.0</td></tr><tr><td>2327</td><td>205784.0</td></tr><tr><td>2328</td><td>224753.0</td></tr><tr><td>2329</td><td>190257.0</td></tr><tr><td>2330</td><td>180773.0</td></tr><tr><td>2331</td><td>346950.0</td></tr><tr><td>2332</td><td>421316.0</td></tr><tr><td>2333</td><td>323250.0</td></tr><tr><td>2334</td><td>255035.0</td></tr><tr><td>2335</td><td>275568.0</td></tr><tr><td>2336</td><td>317633.0</td></tr><tr><td>2337</td><td>201221.0</td></tr><tr><td>2338</td><td>274652.0</td></tr><tr><td>2339</td><td>223409.0</td></tr><tr><td>2340</td><td>391431.0</td></tr><tr><td>2341</td><td>224242.0</td></tr><tr><td>2342</td><td>236565.0</td></tr><tr><td>2343</td><td>225760.0</td></tr><tr><td>2344</td><td>225432.0</td></tr><tr><td>2345</td><td>237890.0</td></tr><tr><td>2346</td><td>212819.0</td></tr><tr><td>2347</td><td>199428.0</td></tr><tr><td>2348</td><td>236966.0</td></tr><tr><td>2349</td><td>178815.0</td></tr><tr><td>2350</td><td>296956.0</td></tr><tr><td>2351</td><td>253357.0</td></tr><tr><td>2352</td><td>257471.0</td></tr><tr><td>2353</td><td>240149.0</td></tr><tr><td>2354</td><td>140742.0</td></tr><tr><td>2355</td><td>147245.0</td></tr><tr><td>2356</td><td>152739.0</td></tr><tr><td>2357</td><td>189459.0</td></tr><tr><td>2358</td><td>196065.0</td></tr><tr><td>2359</td><td>132557.0</td></tr><tr><td>2360</td><td>116236.0</td></tr><tr><td>2361</td><td>147035.0</td></tr><tr><td>2362</td><td>264501.0</td></tr><tr><td>2363</td><td>140338.0</td></tr><tr><td>2364</td><td>157296.0</td></tr><tr><td>2365</td><td>214949.0</td></tr><tr><td>2366</td><td>189179.0</td></tr><tr><td>2367</td><td>223522.0</td></tr><tr><td>2368</td><td>215556.0</td></tr><tr><td>2369</td><td>222210.0</td></tr><tr><td>2370</td><td>168291.0</td></tr><tr><td>2371</td><td>168953.0</td></tr><tr><td>2372</td><td>195597.0</td></tr><tr><td>2373</td><td>284913.0</td></tr><tr><td>2374</td><td>317150.0</td></tr><tr><td>2375</td><td>254211.0</td></tr><tr><td>2376</td><td>277843.0</td></tr><tr><td>2377</td><td>330638.0</td></tr><tr><td>2378</td><td>144629.0</td></tr><tr><td>2379</td><td>197347.0</td></tr><tr><td>2380</td><td>144268.0</td></tr><tr><td>2381</td><td>168457.0</td></tr><tr><td>2382</td><td>209805.0</td></tr><tr><td>2383</td><td>210330.0</td></tr><tr><td>2384</td><td>247463.0</td></tr><tr><td>2385</td><td>164466.0</td></tr><tr><td>2386</td><td>135553.0</td></tr><tr><td>2387</td><td>134901.0</td></tr><tr><td>2388</td><td>101969.0</td></tr><tr><td>2389</td><td>119326.0</td></tr><tr><td>2390</td><td>147776.0</td></tr><tr><td>2391</td><td>141639.0</td></tr><tr><td>2392</td><td>114893.0</td></tr><tr><td>2393</td><td>170237.0</td></tr><tr><td>2394</td><td>145046.0</td></tr><tr><td>2395</td><td>213704.0</td></tr><tr><td>2396</td><td>146714.0</td></tr><tr><td>2397</td><td>224637.0</td></tr><tr><td>2398</td><td>133263.0</td></tr><tr><td>2399</td><td>61131.0</td></tr><tr><td>2400</td><td>58575.0</td></tr><tr><td>2401</td><td>119537.0</td></tr><tr><td>2402</td><td>139227.0</td></tr><tr><td>2403</td><td>141377.0</td></tr><tr><td>2404</td><td>153704.0</td></tr><tr><td>2405</td><td>161828.0</td></tr><tr><td>2406</td><td>140728.0</td></tr><tr><td>2407</td><td>127268.0</td></tr><tr><td>2408</td><td>148817.0</td></tr><tr><td>2409</td><td>123191.0</td></tr><tr><td>2410</td><td>163143.0</td></tr><tr><td>2411</td><td>117492.0</td></tr><tr><td>2412</td><td>158647.0</td></tr><tr><td>2413</td><td>131753.0</td></tr><tr><td>2414</td><td>148627.0</td></tr><tr><td>2415</td><td>131829.0</td></tr><tr><td>2416</td><td>126670.0</td></tr><tr><td>2417</td><td>136127.0</td></tr><tr><td>2418</td><td>133325.0</td></tr><tr><td>2419</td><td>128081.0</td></tr><tr><td>2420</td><td>124153.0</td></tr><tr><td>2421</td><td>142105.0</td></tr><tr><td>2422</td><td>110358.0</td></tr><tr><td>2423</td><td>121122.0</td></tr><tr><td>2424</td><td>151549.0</td></tr><tr><td>2425</td><td>200978.0</td></tr><tr><td>2426</td><td>131658.0</td></tr><tr><td>2427</td><td>134950.0</td></tr><tr><td>2428</td><td>179736.0</td></tr><tr><td>2429</td><td>116096.0</td></tr><tr><td>2430</td><td>133224.0</td></tr><tr><td>2431</td><td>107336.0</td></tr><tr><td>2432</td><td>146190.0</td></tr><tr><td>2433</td><td>150363.0</td></tr><tr><td>2434</td><td>141102.0</td></tr><tr><td>2435</td><td>151883.0</td></tr><tr><td>2436</td><td>104021.0</td></tr><tr><td>2437</td><td>105500.0</td></tr><tr><td>2438</td><td>125074.0</td></tr><tr><td>2439</td><td>102294.0</td></tr><tr><td>2440</td><td>121718.0</td></tr><tr><td>2441</td><td>94367.0</td></tr><tr><td>2442</td><td>96709.0</td></tr><tr><td>2443</td><td>125119.0</td></tr><tr><td>2444</td><td>129623.0</td></tr><tr><td>2445</td><td>86485.0</td></tr><tr><td>2446</td><td>138977.0</td></tr><tr><td>2447</td><td>182065.0</td></tr><tr><td>2448</td><td>128539.0</td></tr><tr><td>2449</td><td>108742.0</td></tr><tr><td>2450</td><td>150539.0</td></tr><tr><td>2451</td><td>121108.0</td></tr><tr><td>2452</td><td>202120.0</td></tr><tr><td>2453</td><td>93222.0</td></tr><tr><td>2454</td><td>121650.0</td></tr><tr><td>2455</td><td>122371.0</td></tr><tr><td>2456</td><td>135326.0</td></tr><tr><td>2457</td><td>132992.0</td></tr><tr><td>2458</td><td>126638.0</td></tr><tr><td>2459</td><td>112721.0</td></tr><tr><td>2460</td><td>142605.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print('Predict submission')\n",
    "submission = pd.read_csv(\"/dbfs/FileStore/tables/sample_submission.csv\")\n",
    "\n",
    "submission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))\n",
    "submission_avg=spark.createDataFrame(submission)\n",
    "display(submission_avg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "Machine Learning",
  "notebookId": 2080443523865599
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
